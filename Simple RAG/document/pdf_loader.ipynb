{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6c879503",
   "metadata": {},
   "source": [
    "### RAG Pipeline : Data Ingestion to Vector DB "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8d6377b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_community.document_loaders import PyPDFLoader, PyMuPDFLoader\n",
    "from langchain_text_splitters.character import RecursiveCharacterTextSplitter\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "88a312dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3 PDF files to process\n",
      "\n",
      "Processing: DJS Compute DA & ML Task 1.pdf\n",
      "  ✓ Loaded 2 pages\n",
      "\n",
      "Processing: DJS Compute DA & ML Task 2.pdf\n",
      "  ✓ Loaded 3 pages\n",
      "\n",
      "Processing: DJS Compute DA & ML Gen AI Task 1.pdf\n",
      "  ✓ Loaded 2 pages\n",
      "\n",
      "Total documents loaded: 7\n"
     ]
    }
   ],
   "source": [
    "# Read all the pdf's inside the directory\n",
    "def process_all_pdfs(pdf_directory):\n",
    "    \"\"\"Process all PDF files in a directory\"\"\"\n",
    "    all_documents = []\n",
    "    pdf_dir = Path(pdf_directory)\n",
    "    \n",
    "    # Find all PDF files recursively\n",
    "    pdf_files = list(pdf_dir.glob(\"**/*.pdf\"))\n",
    "    \n",
    "    print(f\"Found {len(pdf_files)} PDF files to process\")\n",
    "    \n",
    "    for pdf_file in pdf_files:\n",
    "        print(f\"\\nProcessing: {pdf_file.name}\")\n",
    "        try:\n",
    "            loader = PyPDFLoader(str(pdf_file))\n",
    "            documents = loader.load()\n",
    "            \n",
    "            # Add source information to metadata\n",
    "            for doc in documents:\n",
    "                doc.metadata['source_file'] = pdf_file.name\n",
    "                doc.metadata['file_type'] = 'pdf'\n",
    "            \n",
    "            all_documents.extend(documents)\n",
    "            print(f\"  ✓ Loaded {len(documents)} pages\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  ✗ Error: {e}\")\n",
    "    \n",
    "    print(f\"\\nTotal documents loaded: {len(all_documents)}\")\n",
    "    return all_documents\n",
    "\n",
    "# Process all PDFs in the data directory\n",
    "all_pdf_documents = process_all_pdfs(\"../data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "96748c96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'Skia/PDF m142 Google Docs Renderer', 'creator': 'PyPDF', 'creationdate': '', 'title': 'DJS Compute DA & ML Task 1', 'source': '../data/pdf/DJS Compute DA & ML Task 1.pdf', 'total_pages': 2, 'page': 0, 'page_label': '1', 'source_file': 'DJS Compute DA & ML Task 1.pdf', 'file_type': 'pdf'}, page_content='DJS  Compute  -  DA  &  ML  Task  1  \\n \\nData:  https://drive.google.com/file/d/1Dtw-B_hont8b8s_vhl-ecDNNtsHtM91J/view?usp=drive_link \\n \\nTask:  \\n●  Perform  appropriate  preprocessing  and  classification  on  the  given  dataset.   ●  Identify  if  the  data  is  imbalanced  and  apply  appropriate  measures  to  handle  it  like  \\nSMOTE\\n \\nand\\n \\nfeature\\n \\nextraction.\\n \\n ●  Perform  Imputation  based  on  Imputers  available  on  Scikit  Learn.  ●  Use  bagging  and  boosting  techniques.   ●  Perform  LDA  and  QDA  on  the  dataset.  ●  Heavily  reference  the  scikit-learn  documentation  as  well  as  reference  videos  and  use  \\ntechniques\\n \\nlike\\n \\nPipelines,\\n \\nColumn\\n \\nTransformers\\n \\nand\\n \\nhyper\\n \\nparameter\\n \\ntuning.\\n \\n ●  Compare  different  approaches  to  scaling  and  imputing  and  analyze  how  they  affect  the  \\nmodel\\n \\nperformance.\\n \\n ●  Evaluate  the  model  using  multiple  metrics  and  justify  which  ones  are  most  appropriate  to  \\nuse\\n \\nin\\n \\nthe\\n \\ngiven\\n \\nscenario.\\n \\nThe  use  of  comments  to  explain  your  code  is  mandatory.  The  code  can  be  AI  generated.  The  \\ncomments\\n \\nshould\\n \\nnot\\n \\nbe\\n.\\n \\n \\nReferences:  \\nDocumentation:\\n https://imbalanced-learn.org/stable/references/under_sampling.html \\nhttps://imbalanced-learn.org/stable/references/over_sampling.html \\nhttps://scikit-learn.org/stable/api/index.html'),\n",
       " Document(metadata={'producer': 'Skia/PDF m142 Google Docs Renderer', 'creator': 'PyPDF', 'creationdate': '', 'title': 'DJS Compute DA & ML Task 1', 'source': '../data/pdf/DJS Compute DA & ML Task 1.pdf', 'total_pages': 2, 'page': 1, 'page_label': '2', 'source_file': 'DJS Compute DA & ML Task 1.pdf', 'file_type': 'pdf'}, page_content='Videos:  \\nIntro  to  Scikit-Learn  -  https://youtu.be/SIEaLBXr0rk?si=bfIWbYL9zBkjAHPm \\nHyperparameter  Tuning  -  https://www.youtube.com/watch?v=LrCylIe0RJM \\nHandling  Class  Imbalances  -  https://youtu.be/flhjn6e6wnY?si=ELzIHiHarhF488cd \\nIntro  to  sklearn-ColumnTransformer  -  https://youtu.be/yBLNbeKbFKI?si=Gv3J0g-Wb_ZpijBz \\nIntro  to  sklearn-Pipeline  -  https://www.youtube.com/watch?v=T9ETsSD1I0w \\nFeature  Scaling  -  https://www.youtube.com/watch?v=6eJHk8JYK2M \\nSimpleImputer  -  https://www.youtube.com/watch?v=0Hsj0h-tXAY \\nClassification  Metrics  -  https://www.youtube.com/watch?v=5vqk6HnITko&list=PLiteiKUvOPTelAIiquq5-VKFguUJpv1BT \\nLDA  -  https://youtu.be/azXCzI57Yfc?si=TT7TW_Yk5I0dncS_ \\n \\n \\n \\nDeadline:  30th  September,  2025'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'Microsoft Word', 'creationdate': '2025-10-05T13:26:17+00:00', 'moddate': '2025-10-05T13:26:17+00:00', 'source': '../data/pdf/DJS Compute DA & ML Task 2.pdf', 'total_pages': 3, 'page': 0, 'page_label': '1', 'source_file': 'DJS Compute DA & ML Task 2.pdf', 'file_type': 'pdf'}, page_content='DJS Compute - DA & ML Task 2 \\n \\nData : \\nhttps://drive.google.com/file/d/1dJEbm6rSl1pr0mH5d0k5_WXHsSjGMAZs/view?usp=drivesdk \\n \\nTask: \\n⇒ PART ONE: \\n● Perform appropriate preprocessing and deep EDA on the given dataset, just like the \\nprevious task. This includes things like handling Nans, analyzing relationships, feature \\nencoding, Scaling, Normalizing and other similar methods.   \\n● Before Clustering, use PCA(Principal Component Analysis) as a dimensionality \\nreduction technique. For better intuition, you can set the n_components parameter to 2/3 \\nand then plot the Principal Component axes using a 2D/3D scatterplot  to scout for \\npotential clusters. \\n● Perform the following clustering techniques: \\ni) K-Means: Tune the number of clusters (k) using the Elbow method and Silhouette score \\nplots. \\nii) Hierarchical Clustering: Use a dendrogram to visually determine the optimal number of \\nclusters. \\niii) DBSCAN: Use a k-distance plot to find an appropriate value for the eps parameter. \\niv) HDBSCAN: Tune min_cluster_size by setting it to the smallest meaningful group size, \\nor by finding the value that maximizes the DBCV score. \\n● For every clustering experiment you run, calculate a suite of quantitative metrics \\nincluding the Silhouette Score, Davies-Bouldin Index (DBI), and Calinski-Harabasz (CH) \\nScore. \\n● For your single best-performing model, move beyond metrics to interpretation. Profile \\neach cluster by calculating its central tendencies (mean/median for numeric features, \\nmode for categorical features) and visualize these differences to create insightful, \\ndescriptive personas (Ex:- Cluster 1: Cautious High-Earners).'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'Microsoft Word', 'creationdate': '2025-10-05T13:26:17+00:00', 'moddate': '2025-10-05T13:26:17+00:00', 'source': '../data/pdf/DJS Compute DA & ML Task 2.pdf', 'total_pages': 3, 'page': 1, 'page_label': '2', 'source_file': 'DJS Compute DA & ML Task 2.pdf', 'file_type': 'pdf'}, page_content=\"⇒ PART TWO: \\nBefore building the final application, you will validate your understanding of clustering \\nalgorithms by testing them on simple, synthetic datasets where the ground truth is known. \\n● Create synthetic datasets using sklearn functions like make_blobs/make_moons and be \\nsure to experiment with their parameters as well. Heavily reference the sklearn \\ndocumentation for getting familiar with all the parameters. \\n● For more clarity, use visual plots to see how different parameter combinations affect the \\ncluster formations. \\n● STREAMLIT DEPLOYMENT: \\nThe app that you make should reference the synthetic dataset which you created and must \\nhave the following deliverables: \\n○ Feature Scaling Options: Include a select box for preprocessing, allowing a choice \\nbetween different scalers like 'StandardScaler', 'MinMaxScaler', 'RobustScaler', or 'None'. \\n○ Algorithm Choice: Create a dropdown menu for selecting the clustering algorithm (Ex:-\\n'K-Means', 'DBSCAN', 'Agglomerative', 'HDBSCAN'). \\n○ Dynamic Hyperparameters: Display interactive widgets (sliders are best) for the chosen \\nalgorithm's specific parameters, such as k for K-Means or eps for DBSCAN,etc. \\n○ Interactive Visualization: Render the output as a Plotly scatter plot where each point's \\ncolor corresponds to its assigned cluster label. \\n○ Performance Metrics: Show key evaluation metrics like the Silhouette Score to provide \\na quantitative measure of the clustering quality. \\n- As you wanna view the Plotly scatterplot in 2D/3D make sure to manipulate the - \\n‘n_features’ parameter accordingly while making you synthetic dataset. \\n‘make_moons’ can only create a 2D dataset, so you don’t need to worry about that if \\nyou’re using this function. \\n- NOTE : Keep the datapoint range for the synthetic dataset preferably between 500-\\n2000, as we don’t want unnecessary latency in the deployed app. \\n- The Streamlit deployment is only to be done for PART TWO. \\n \\nThe use of comments to explain your code is mandatory. The code can be AI generated. The \\ncomments should not be.\"),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'Microsoft Word', 'creationdate': '2025-10-05T13:26:17+00:00', 'moddate': '2025-10-05T13:26:17+00:00', 'source': '../data/pdf/DJS Compute DA & ML Task 2.pdf', 'total_pages': 3, 'page': 2, 'page_label': '3', 'source_file': 'DJS Compute DA & ML Task 2.pdf', 'file_type': 'pdf'}, page_content='References: \\nDocumentation: \\nRefer directly from scikit-learn’s website. \\n \\nVideos: \\nPCA : https://www.youtube.com/watch?v=FD4DeN81ODY \\nIntro to Unsupervised : https://www.youtube.com/watch?v=5yeJ03crTrI \\nK-Means : https://www.youtube.com/watch?v=4b5d3muPQmA \\nHierarchical : https://www.youtube.com/watch?v=7xHsRkOdVwo \\nDBSCAN : https://www.youtube.com/watch?v=RDZUdRSDOok \\nHDBSCAN : https://www.youtube.com/watch?v=dGsxd67IFiU \\nElbow Method : https://www.youtube.com/watch?v=ht7geyMAFfA \\nStreamlit Deployment : https://www.youtube.com/watch?v=D0D4Pa22iG0&t=603s \\n \\n \\n \\nDeadline: 14th October, 2025'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'Microsoft Word', 'creationdate': '2025-10-20T17:40:42+00:00', 'moddate': '2025-10-20T17:40:42+00:00', 'source': '../data/pdf/DJS Compute DA & ML Gen AI Task 1.pdf', 'total_pages': 2, 'page': 0, 'page_label': '1', 'source_file': 'DJS Compute DA & ML Gen AI Task 1.pdf', 'file_type': 'pdf'}, page_content='DJS Compute - DA & ML : Gen AI Task 1 \\n \\nLEARNING: \\nThings about LLMS: \\n- Tokenization  \\n- Parameters (Model size)  \\n- Context windows  \\n- System prompt v/s User prompt  \\n- Few shot prompting  \\n \\nWhere to find and run LLMs: \\n- Open source v/s Closed source LLMs - \\nhttps://www.youtube.com/watch?v=710PDpuLwOc \\nhttps://www.youtube.com/watch?v=QQ1bnrTchYM \\n- Huggingface v/s Ollama  \\n- APIs  \\n \\nThe practical side: \\n- API pricing for LLMs (Important)  \\n- Base vs Instruct-tuned LLMs  \\n- Maximizing LLM performance (Very Important) -'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'Microsoft Word', 'creationdate': '2025-10-20T17:40:42+00:00', 'moddate': '2025-10-20T17:40:42+00:00', 'source': '../data/pdf/DJS Compute DA & ML Gen AI Task 1.pdf', 'total_pages': 2, 'page': 1, 'page_label': '2', 'source_file': 'DJS Compute DA & ML Gen AI Task 1.pdf', 'file_type': 'pdf'}, page_content='LLM Leaderboards to check out: \\n- Vellum  \\n- Huggingface OpenLLM  \\n \\n \\nTASK : \\n● Experiment creating a chatbot using different inference methods: \\n- Ollama(locally) \\n- Huggingface transformers (Library) \\n- Gemini/Groq API \\n● REFERENCES : \\n- Ollama \\n- Huggingface Transformers \\n- Groq \\n- Gemini \\n \\nNOTE : The procedure to call and use LLMs even through different \\nmethods is pretty similar and once you do it a couple of times, it’s pretty \\nintuitive. What’s important is knowing the difference between using \\nmodels through UI as a consumer v/s What LLM APIs are, and how \\ntheir inference costs work. After completing this task, also try to \\nintegrate ‘Gradio’ using AI, which is just like Streamlit but for Chatbots. \\n \\nDeadline: 31st October, 2025')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_pdf_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fab6b6cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Text splitting get into chunks\n",
    "\n",
    "def split_documents(documents,chunk_size=1000,chunk_overlap=200):\n",
    "    \"\"\"Split documents into smaller chunks for better RAG performance\"\"\"\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        length_function=len,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    "    )\n",
    "    split_docs = text_splitter.split_documents(documents)\n",
    "    print(f\"Split {len(documents)} documents into {len(split_docs)} chunks\")\n",
    "    \n",
    "    # Show example of a chunk\n",
    "    if split_docs:\n",
    "        print(f\"\\nExample chunk:\")\n",
    "        print(f\"Content: {split_docs[0].page_content[:200]}...\")\n",
    "        print(f\"Metadata: {split_docs[0].metadata}\")\n",
    "    \n",
    "    return split_docs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c7b75a72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split 7 documents into 11 chunks\n",
      "\n",
      "Example chunk:\n",
      "Content: DJS  Compute  -  DA  &  ML  Task  1  \n",
      " \n",
      "Data:  https://drive.google.com/file/d/1Dtw-B_hont8b8s_vhl-ecDNNtsHtM91J/view?usp=drive_link \n",
      " \n",
      "Task:  \n",
      "●  Perform  appropriate  preprocessing  and  classificat...\n",
      "Metadata: {'producer': 'Skia/PDF m142 Google Docs Renderer', 'creator': 'PyPDF', 'creationdate': '', 'title': 'DJS Compute DA & ML Task 1', 'source': '../data/pdf/DJS Compute DA & ML Task 1.pdf', 'total_pages': 2, 'page': 0, 'page_label': '1', 'source_file': 'DJS Compute DA & ML Task 1.pdf', 'file_type': 'pdf'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'Skia/PDF m142 Google Docs Renderer', 'creator': 'PyPDF', 'creationdate': '', 'title': 'DJS Compute DA & ML Task 1', 'source': '../data/pdf/DJS Compute DA & ML Task 1.pdf', 'total_pages': 2, 'page': 0, 'page_label': '1', 'source_file': 'DJS Compute DA & ML Task 1.pdf', 'file_type': 'pdf'}, page_content='DJS  Compute  -  DA  &  ML  Task  1  \\n \\nData:  https://drive.google.com/file/d/1Dtw-B_hont8b8s_vhl-ecDNNtsHtM91J/view?usp=drive_link \\n \\nTask:  \\n●  Perform  appropriate  preprocessing  and  classification  on  the  given  dataset.   ●  Identify  if  the  data  is  imbalanced  and  apply  appropriate  measures  to  handle  it  like  \\nSMOTE\\n \\nand\\n \\nfeature\\n \\nextraction.\\n \\n ●  Perform  Imputation  based  on  Imputers  available  on  Scikit  Learn.  ●  Use  bagging  and  boosting  techniques.   ●  Perform  LDA  and  QDA  on  the  dataset.  ●  Heavily  reference  the  scikit-learn  documentation  as  well  as  reference  videos  and  use  \\ntechniques\\n \\nlike\\n \\nPipelines,\\n \\nColumn\\n \\nTransformers\\n \\nand\\n \\nhyper\\n \\nparameter\\n \\ntuning.\\n \\n ●  Compare  different  approaches  to  scaling  and  imputing  and  analyze  how  they  affect  the  \\nmodel\\n \\nperformance.\\n \\n ●  Evaluate  the  model  using  multiple  metrics  and  justify  which  ones  are  most  appropriate  to  \\nuse\\n \\nin\\n \\nthe\\n \\ngiven'),\n",
       " Document(metadata={'producer': 'Skia/PDF m142 Google Docs Renderer', 'creator': 'PyPDF', 'creationdate': '', 'title': 'DJS Compute DA & ML Task 1', 'source': '../data/pdf/DJS Compute DA & ML Task 1.pdf', 'total_pages': 2, 'page': 0, 'page_label': '1', 'source_file': 'DJS Compute DA & ML Task 1.pdf', 'file_type': 'pdf'}, page_content='model\\n \\nperformance.\\n \\n ●  Evaluate  the  model  using  multiple  metrics  and  justify  which  ones  are  most  appropriate  to  \\nuse\\n \\nin\\n \\nthe\\n \\ngiven\\n \\nscenario.\\n \\nThe  use  of  comments  to  explain  your  code  is  mandatory.  The  code  can  be  AI  generated.  The  \\ncomments\\n \\nshould\\n \\nnot\\n \\nbe\\n.\\n \\n \\nReferences:  \\nDocumentation:\\n https://imbalanced-learn.org/stable/references/under_sampling.html \\nhttps://imbalanced-learn.org/stable/references/over_sampling.html \\nhttps://scikit-learn.org/stable/api/index.html'),\n",
       " Document(metadata={'producer': 'Skia/PDF m142 Google Docs Renderer', 'creator': 'PyPDF', 'creationdate': '', 'title': 'DJS Compute DA & ML Task 1', 'source': '../data/pdf/DJS Compute DA & ML Task 1.pdf', 'total_pages': 2, 'page': 1, 'page_label': '2', 'source_file': 'DJS Compute DA & ML Task 1.pdf', 'file_type': 'pdf'}, page_content='Videos:  \\nIntro  to  Scikit-Learn  -  https://youtu.be/SIEaLBXr0rk?si=bfIWbYL9zBkjAHPm \\nHyperparameter  Tuning  -  https://www.youtube.com/watch?v=LrCylIe0RJM \\nHandling  Class  Imbalances  -  https://youtu.be/flhjn6e6wnY?si=ELzIHiHarhF488cd \\nIntro  to  sklearn-ColumnTransformer  -  https://youtu.be/yBLNbeKbFKI?si=Gv3J0g-Wb_ZpijBz \\nIntro  to  sklearn-Pipeline  -  https://www.youtube.com/watch?v=T9ETsSD1I0w \\nFeature  Scaling  -  https://www.youtube.com/watch?v=6eJHk8JYK2M \\nSimpleImputer  -  https://www.youtube.com/watch?v=0Hsj0h-tXAY \\nClassification  Metrics  -  https://www.youtube.com/watch?v=5vqk6HnITko&list=PLiteiKUvOPTelAIiquq5-VKFguUJpv1BT \\nLDA  -  https://youtu.be/azXCzI57Yfc?si=TT7TW_Yk5I0dncS_ \\n \\n \\n \\nDeadline:  30th  September,  2025'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'Microsoft Word', 'creationdate': '2025-10-05T13:26:17+00:00', 'moddate': '2025-10-05T13:26:17+00:00', 'source': '../data/pdf/DJS Compute DA & ML Task 2.pdf', 'total_pages': 3, 'page': 0, 'page_label': '1', 'source_file': 'DJS Compute DA & ML Task 2.pdf', 'file_type': 'pdf'}, page_content='DJS Compute - DA & ML Task 2 \\n \\nData : \\nhttps://drive.google.com/file/d/1dJEbm6rSl1pr0mH5d0k5_WXHsSjGMAZs/view?usp=drivesdk \\n \\nTask: \\n⇒ PART ONE: \\n● Perform appropriate preprocessing and deep EDA on the given dataset, just like the \\nprevious task. This includes things like handling Nans, analyzing relationships, feature \\nencoding, Scaling, Normalizing and other similar methods.   \\n● Before Clustering, use PCA(Principal Component Analysis) as a dimensionality \\nreduction technique. For better intuition, you can set the n_components parameter to 2/3 \\nand then plot the Principal Component axes using a 2D/3D scatterplot  to scout for \\npotential clusters. \\n● Perform the following clustering techniques: \\ni) K-Means: Tune the number of clusters (k) using the Elbow method and Silhouette score \\nplots. \\nii) Hierarchical Clustering: Use a dendrogram to visually determine the optimal number of \\nclusters. \\niii) DBSCAN: Use a k-distance plot to find an appropriate value for the eps parameter.'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'Microsoft Word', 'creationdate': '2025-10-05T13:26:17+00:00', 'moddate': '2025-10-05T13:26:17+00:00', 'source': '../data/pdf/DJS Compute DA & ML Task 2.pdf', 'total_pages': 3, 'page': 0, 'page_label': '1', 'source_file': 'DJS Compute DA & ML Task 2.pdf', 'file_type': 'pdf'}, page_content='plots. \\nii) Hierarchical Clustering: Use a dendrogram to visually determine the optimal number of \\nclusters. \\niii) DBSCAN: Use a k-distance plot to find an appropriate value for the eps parameter. \\niv) HDBSCAN: Tune min_cluster_size by setting it to the smallest meaningful group size, \\nor by finding the value that maximizes the DBCV score. \\n● For every clustering experiment you run, calculate a suite of quantitative metrics \\nincluding the Silhouette Score, Davies-Bouldin Index (DBI), and Calinski-Harabasz (CH) \\nScore. \\n● For your single best-performing model, move beyond metrics to interpretation. Profile \\neach cluster by calculating its central tendencies (mean/median for numeric features, \\nmode for categorical features) and visualize these differences to create insightful, \\ndescriptive personas (Ex:- Cluster 1: Cautious High-Earners).'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'Microsoft Word', 'creationdate': '2025-10-05T13:26:17+00:00', 'moddate': '2025-10-05T13:26:17+00:00', 'source': '../data/pdf/DJS Compute DA & ML Task 2.pdf', 'total_pages': 3, 'page': 1, 'page_label': '2', 'source_file': 'DJS Compute DA & ML Task 2.pdf', 'file_type': 'pdf'}, page_content=\"⇒ PART TWO: \\nBefore building the final application, you will validate your understanding of clustering \\nalgorithms by testing them on simple, synthetic datasets where the ground truth is known. \\n● Create synthetic datasets using sklearn functions like make_blobs/make_moons and be \\nsure to experiment with their parameters as well. Heavily reference the sklearn \\ndocumentation for getting familiar with all the parameters. \\n● For more clarity, use visual plots to see how different parameter combinations affect the \\ncluster formations. \\n● STREAMLIT DEPLOYMENT: \\nThe app that you make should reference the synthetic dataset which you created and must \\nhave the following deliverables: \\n○ Feature Scaling Options: Include a select box for preprocessing, allowing a choice \\nbetween different scalers like 'StandardScaler', 'MinMaxScaler', 'RobustScaler', or 'None'. \\n○ Algorithm Choice: Create a dropdown menu for selecting the clustering algorithm (Ex:-\"),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'Microsoft Word', 'creationdate': '2025-10-05T13:26:17+00:00', 'moddate': '2025-10-05T13:26:17+00:00', 'source': '../data/pdf/DJS Compute DA & ML Task 2.pdf', 'total_pages': 3, 'page': 1, 'page_label': '2', 'source_file': 'DJS Compute DA & ML Task 2.pdf', 'file_type': 'pdf'}, page_content=\"between different scalers like 'StandardScaler', 'MinMaxScaler', 'RobustScaler', or 'None'. \\n○ Algorithm Choice: Create a dropdown menu for selecting the clustering algorithm (Ex:-\\n'K-Means', 'DBSCAN', 'Agglomerative', 'HDBSCAN'). \\n○ Dynamic Hyperparameters: Display interactive widgets (sliders are best) for the chosen \\nalgorithm's specific parameters, such as k for K-Means or eps for DBSCAN,etc. \\n○ Interactive Visualization: Render the output as a Plotly scatter plot where each point's \\ncolor corresponds to its assigned cluster label. \\n○ Performance Metrics: Show key evaluation metrics like the Silhouette Score to provide \\na quantitative measure of the clustering quality. \\n- As you wanna view the Plotly scatterplot in 2D/3D make sure to manipulate the - \\n‘n_features’ parameter accordingly while making you synthetic dataset. \\n‘make_moons’ can only create a 2D dataset, so you don’t need to worry about that if \\nyou’re using this function.\"),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'Microsoft Word', 'creationdate': '2025-10-05T13:26:17+00:00', 'moddate': '2025-10-05T13:26:17+00:00', 'source': '../data/pdf/DJS Compute DA & ML Task 2.pdf', 'total_pages': 3, 'page': 1, 'page_label': '2', 'source_file': 'DJS Compute DA & ML Task 2.pdf', 'file_type': 'pdf'}, page_content='‘n_features’ parameter accordingly while making you synthetic dataset. \\n‘make_moons’ can only create a 2D dataset, so you don’t need to worry about that if \\nyou’re using this function. \\n- NOTE : Keep the datapoint range for the synthetic dataset preferably between 500-\\n2000, as we don’t want unnecessary latency in the deployed app. \\n- The Streamlit deployment is only to be done for PART TWO. \\n \\nThe use of comments to explain your code is mandatory. The code can be AI generated. The \\ncomments should not be.'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'Microsoft Word', 'creationdate': '2025-10-05T13:26:17+00:00', 'moddate': '2025-10-05T13:26:17+00:00', 'source': '../data/pdf/DJS Compute DA & ML Task 2.pdf', 'total_pages': 3, 'page': 2, 'page_label': '3', 'source_file': 'DJS Compute DA & ML Task 2.pdf', 'file_type': 'pdf'}, page_content='References: \\nDocumentation: \\nRefer directly from scikit-learn’s website. \\n \\nVideos: \\nPCA : https://www.youtube.com/watch?v=FD4DeN81ODY \\nIntro to Unsupervised : https://www.youtube.com/watch?v=5yeJ03crTrI \\nK-Means : https://www.youtube.com/watch?v=4b5d3muPQmA \\nHierarchical : https://www.youtube.com/watch?v=7xHsRkOdVwo \\nDBSCAN : https://www.youtube.com/watch?v=RDZUdRSDOok \\nHDBSCAN : https://www.youtube.com/watch?v=dGsxd67IFiU \\nElbow Method : https://www.youtube.com/watch?v=ht7geyMAFfA \\nStreamlit Deployment : https://www.youtube.com/watch?v=D0D4Pa22iG0&t=603s \\n \\n \\n \\nDeadline: 14th October, 2025'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'Microsoft Word', 'creationdate': '2025-10-20T17:40:42+00:00', 'moddate': '2025-10-20T17:40:42+00:00', 'source': '../data/pdf/DJS Compute DA & ML Gen AI Task 1.pdf', 'total_pages': 2, 'page': 0, 'page_label': '1', 'source_file': 'DJS Compute DA & ML Gen AI Task 1.pdf', 'file_type': 'pdf'}, page_content='DJS Compute - DA & ML : Gen AI Task 1 \\n \\nLEARNING: \\nThings about LLMS: \\n- Tokenization  \\n- Parameters (Model size)  \\n- Context windows  \\n- System prompt v/s User prompt  \\n- Few shot prompting  \\n \\nWhere to find and run LLMs: \\n- Open source v/s Closed source LLMs - \\nhttps://www.youtube.com/watch?v=710PDpuLwOc \\nhttps://www.youtube.com/watch?v=QQ1bnrTchYM \\n- Huggingface v/s Ollama  \\n- APIs  \\n \\nThe practical side: \\n- API pricing for LLMs (Important)  \\n- Base vs Instruct-tuned LLMs  \\n- Maximizing LLM performance (Very Important) -'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'Microsoft Word', 'creationdate': '2025-10-20T17:40:42+00:00', 'moddate': '2025-10-20T17:40:42+00:00', 'source': '../data/pdf/DJS Compute DA & ML Gen AI Task 1.pdf', 'total_pages': 2, 'page': 1, 'page_label': '2', 'source_file': 'DJS Compute DA & ML Gen AI Task 1.pdf', 'file_type': 'pdf'}, page_content='LLM Leaderboards to check out: \\n- Vellum  \\n- Huggingface OpenLLM  \\n \\n \\nTASK : \\n● Experiment creating a chatbot using different inference methods: \\n- Ollama(locally) \\n- Huggingface transformers (Library) \\n- Gemini/Groq API \\n● REFERENCES : \\n- Ollama \\n- Huggingface Transformers \\n- Groq \\n- Gemini \\n \\nNOTE : The procedure to call and use LLMs even through different \\nmethods is pretty similar and once you do it a couple of times, it’s pretty \\nintuitive. What’s important is knowing the difference between using \\nmodels through UI as a consumer v/s What LLM APIs are, and how \\ntheir inference costs work. After completing this task, also try to \\nintegrate ‘Gradio’ using AI, which is just like Streamlit but for Chatbots. \\n \\nDeadline: 31st October, 2025')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks=split_documents(all_pdf_documents)\n",
    "chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97fe92ea",
   "metadata": {},
   "source": [
    "### Embedding And VectorDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e3ae3031",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "import uuid\n",
    "from typing import List, Dict, Any, Tuple\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "543614c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading embedding model: all-MiniLM-L6-v2\n",
      "Model loaded successfully. Embedding dimension: 384\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.EmbeddingManager at 0x147be7620>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class EmbeddingManager:\n",
    "    \"\"\"Handles document embedding generation using SentenceTransformer\"\"\"\n",
    "    \n",
    "    def __init__(self, model_name: str = \"all-MiniLM-L6-v2\"):\n",
    "        \"\"\"\n",
    "        Initialize the embedding manager\n",
    "        \n",
    "        Args:\n",
    "            model_name: HuggingFace model name for sentence embeddings\n",
    "        \"\"\"\n",
    "        self.model_name = model_name\n",
    "        self.model = None\n",
    "        self._load_model()\n",
    "\n",
    "    def _load_model(self):\n",
    "        \"\"\"Load the SentenceTransformer model\"\"\"\n",
    "        try:\n",
    "            print(f\"Loading embedding model: {self.model_name}\")\n",
    "            self.model = SentenceTransformer(self.model_name)\n",
    "            print(f\"Model loaded successfully. Embedding dimension: {self.model.get_sentence_embedding_dimension()}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading model {self.model_name}: {e}\")\n",
    "            raise\n",
    "\n",
    "    def generate_embeddings(self, texts: List[str]) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Generate embeddings for a list of texts\n",
    "        \n",
    "        Args:\n",
    "            texts: List of text strings to embed\n",
    "            \n",
    "        Returns:\n",
    "            numpy array of embeddings with shape (len(texts), embedding_dim)\n",
    "        \"\"\"\n",
    "        if not self.model:\n",
    "            raise ValueError(\"Model not loaded\")\n",
    "        \n",
    "        print(f\"Generating embeddings for {len(texts)} texts...\")\n",
    "        embeddings = self.model.encode(texts, show_progress_bar=True)\n",
    "        print(f\"Generated embeddings with shape: {embeddings.shape}\")\n",
    "        return embeddings\n",
    "\n",
    "\n",
    "## initialize the embedding manager\n",
    "\n",
    "embedding_manager=EmbeddingManager()\n",
    "embedding_manager\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f62c9e3b",
   "metadata": {},
   "source": [
    "### VectorStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c276d1ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector store initialized. Collection: pdf_documents\n",
      "Existing documents in collection: 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.VectorStore at 0x15dad2e40>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class VectorStore:\n",
    "    \"\"\"Manages document embeddings in a ChromaDB vector store\"\"\"\n",
    "    \n",
    "    def __init__(self, collection_name: str = \"pdf_documents\", persist_directory: str = \"../data/vector_store\"):\n",
    "        \"\"\"\n",
    "        Initialize the vector store\n",
    "        \n",
    "        Args:\n",
    "            collection_name: Name of the ChromaDB collection\n",
    "            persist_directory: Directory to persist the vector store\n",
    "        \"\"\"\n",
    "        self.collection_name = collection_name\n",
    "        self.persist_directory = persist_directory\n",
    "        self.client = None\n",
    "        self.collection = None\n",
    "        self._initialize_store()\n",
    "\n",
    "    def _initialize_store(self):\n",
    "        \"\"\"Initialize ChromaDB client and collection\"\"\"\n",
    "        try:\n",
    "            # Create persistent ChromaDB client\n",
    "            os.makedirs(self.persist_directory, exist_ok=True)\n",
    "            self.client = chromadb.PersistentClient(path=self.persist_directory)\n",
    "            \n",
    "            # Get or create collection\n",
    "            self.collection = self.client.get_or_create_collection(\n",
    "                name=self.collection_name,\n",
    "                metadata={\"description\": \"PDF document embeddings for RAG\"}\n",
    "            )\n",
    "            print(f\"Vector store initialized. Collection: {self.collection_name}\")\n",
    "            print(f\"Existing documents in collection: {self.collection.count()}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error initializing vector store: {e}\")\n",
    "            raise\n",
    "\n",
    "    def add_documents(self, documents: List[Any], embeddings: np.ndarray):\n",
    "        \"\"\"\n",
    "        Add documents and their embeddings to the vector store\n",
    "        \n",
    "        Args:\n",
    "            documents: List of LangChain documents\n",
    "            embeddings: Corresponding embeddings for the documents\n",
    "        \"\"\"\n",
    "        if len(documents) != len(embeddings):\n",
    "            raise ValueError(\"Number of documents must match number of embeddings\")\n",
    "        \n",
    "        print(f\"Adding {len(documents)} documents to vector store...\")\n",
    "        \n",
    "        # Prepare data for ChromaDB\n",
    "        ids = []\n",
    "        metadatas = []\n",
    "        documents_text = []\n",
    "        embeddings_list = []\n",
    "        \n",
    "        for i, (doc, embedding) in enumerate(zip(documents, embeddings)):\n",
    "            # Generate unique ID\n",
    "            doc_id = f\"doc_{uuid.uuid4().hex[:8]}_{i}\"\n",
    "            ids.append(doc_id)\n",
    "            \n",
    "            # Prepare metadata\n",
    "            metadata = dict(doc.metadata)\n",
    "            metadata['doc_index'] = i\n",
    "            metadata['content_length'] = len(doc.page_content)\n",
    "            metadatas.append(metadata)\n",
    "            \n",
    "            # Document content\n",
    "            documents_text.append(doc.page_content)\n",
    "            \n",
    "            # Embedding\n",
    "            embeddings_list.append(embedding.tolist())\n",
    "        \n",
    "        # Add to collection\n",
    "        try:\n",
    "            self.collection.add(\n",
    "                ids=ids,\n",
    "                embeddings=embeddings_list,\n",
    "                metadatas=metadatas,\n",
    "                documents=documents_text\n",
    "            )\n",
    "            print(f\"Successfully added {len(documents)} documents to vector store\")\n",
    "            print(f\"Total documents in collection: {self.collection.count()}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error adding documents to vector store: {e}\")\n",
    "            raise\n",
    "\n",
    "vectorstore=VectorStore()\n",
    "vectorstore\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2d5d2c88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'Skia/PDF m142 Google Docs Renderer', 'creator': 'PyPDF', 'creationdate': '', 'title': 'DJS Compute DA & ML Task 1', 'source': '../data/pdf/DJS Compute DA & ML Task 1.pdf', 'total_pages': 2, 'page': 0, 'page_label': '1', 'source_file': 'DJS Compute DA & ML Task 1.pdf', 'file_type': 'pdf'}, page_content='DJS  Compute  -  DA  &  ML  Task  1  \\n \\nData:  https://drive.google.com/file/d/1Dtw-B_hont8b8s_vhl-ecDNNtsHtM91J/view?usp=drive_link \\n \\nTask:  \\n●  Perform  appropriate  preprocessing  and  classification  on  the  given  dataset.   ●  Identify  if  the  data  is  imbalanced  and  apply  appropriate  measures  to  handle  it  like  \\nSMOTE\\n \\nand\\n \\nfeature\\n \\nextraction.\\n \\n ●  Perform  Imputation  based  on  Imputers  available  on  Scikit  Learn.  ●  Use  bagging  and  boosting  techniques.   ●  Perform  LDA  and  QDA  on  the  dataset.  ●  Heavily  reference  the  scikit-learn  documentation  as  well  as  reference  videos  and  use  \\ntechniques\\n \\nlike\\n \\nPipelines,\\n \\nColumn\\n \\nTransformers\\n \\nand\\n \\nhyper\\n \\nparameter\\n \\ntuning.\\n \\n ●  Compare  different  approaches  to  scaling  and  imputing  and  analyze  how  they  affect  the  \\nmodel\\n \\nperformance.\\n \\n ●  Evaluate  the  model  using  multiple  metrics  and  justify  which  ones  are  most  appropriate  to  \\nuse\\n \\nin\\n \\nthe\\n \\ngiven'),\n",
       " Document(metadata={'producer': 'Skia/PDF m142 Google Docs Renderer', 'creator': 'PyPDF', 'creationdate': '', 'title': 'DJS Compute DA & ML Task 1', 'source': '../data/pdf/DJS Compute DA & ML Task 1.pdf', 'total_pages': 2, 'page': 0, 'page_label': '1', 'source_file': 'DJS Compute DA & ML Task 1.pdf', 'file_type': 'pdf'}, page_content='model\\n \\nperformance.\\n \\n ●  Evaluate  the  model  using  multiple  metrics  and  justify  which  ones  are  most  appropriate  to  \\nuse\\n \\nin\\n \\nthe\\n \\ngiven\\n \\nscenario.\\n \\nThe  use  of  comments  to  explain  your  code  is  mandatory.  The  code  can  be  AI  generated.  The  \\ncomments\\n \\nshould\\n \\nnot\\n \\nbe\\n.\\n \\n \\nReferences:  \\nDocumentation:\\n https://imbalanced-learn.org/stable/references/under_sampling.html \\nhttps://imbalanced-learn.org/stable/references/over_sampling.html \\nhttps://scikit-learn.org/stable/api/index.html'),\n",
       " Document(metadata={'producer': 'Skia/PDF m142 Google Docs Renderer', 'creator': 'PyPDF', 'creationdate': '', 'title': 'DJS Compute DA & ML Task 1', 'source': '../data/pdf/DJS Compute DA & ML Task 1.pdf', 'total_pages': 2, 'page': 1, 'page_label': '2', 'source_file': 'DJS Compute DA & ML Task 1.pdf', 'file_type': 'pdf'}, page_content='Videos:  \\nIntro  to  Scikit-Learn  -  https://youtu.be/SIEaLBXr0rk?si=bfIWbYL9zBkjAHPm \\nHyperparameter  Tuning  -  https://www.youtube.com/watch?v=LrCylIe0RJM \\nHandling  Class  Imbalances  -  https://youtu.be/flhjn6e6wnY?si=ELzIHiHarhF488cd \\nIntro  to  sklearn-ColumnTransformer  -  https://youtu.be/yBLNbeKbFKI?si=Gv3J0g-Wb_ZpijBz \\nIntro  to  sklearn-Pipeline  -  https://www.youtube.com/watch?v=T9ETsSD1I0w \\nFeature  Scaling  -  https://www.youtube.com/watch?v=6eJHk8JYK2M \\nSimpleImputer  -  https://www.youtube.com/watch?v=0Hsj0h-tXAY \\nClassification  Metrics  -  https://www.youtube.com/watch?v=5vqk6HnITko&list=PLiteiKUvOPTelAIiquq5-VKFguUJpv1BT \\nLDA  -  https://youtu.be/azXCzI57Yfc?si=TT7TW_Yk5I0dncS_ \\n \\n \\n \\nDeadline:  30th  September,  2025'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'Microsoft Word', 'creationdate': '2025-10-05T13:26:17+00:00', 'moddate': '2025-10-05T13:26:17+00:00', 'source': '../data/pdf/DJS Compute DA & ML Task 2.pdf', 'total_pages': 3, 'page': 0, 'page_label': '1', 'source_file': 'DJS Compute DA & ML Task 2.pdf', 'file_type': 'pdf'}, page_content='DJS Compute - DA & ML Task 2 \\n \\nData : \\nhttps://drive.google.com/file/d/1dJEbm6rSl1pr0mH5d0k5_WXHsSjGMAZs/view?usp=drivesdk \\n \\nTask: \\n⇒ PART ONE: \\n● Perform appropriate preprocessing and deep EDA on the given dataset, just like the \\nprevious task. This includes things like handling Nans, analyzing relationships, feature \\nencoding, Scaling, Normalizing and other similar methods.   \\n● Before Clustering, use PCA(Principal Component Analysis) as a dimensionality \\nreduction technique. For better intuition, you can set the n_components parameter to 2/3 \\nand then plot the Principal Component axes using a 2D/3D scatterplot  to scout for \\npotential clusters. \\n● Perform the following clustering techniques: \\ni) K-Means: Tune the number of clusters (k) using the Elbow method and Silhouette score \\nplots. \\nii) Hierarchical Clustering: Use a dendrogram to visually determine the optimal number of \\nclusters. \\niii) DBSCAN: Use a k-distance plot to find an appropriate value for the eps parameter.'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'Microsoft Word', 'creationdate': '2025-10-05T13:26:17+00:00', 'moddate': '2025-10-05T13:26:17+00:00', 'source': '../data/pdf/DJS Compute DA & ML Task 2.pdf', 'total_pages': 3, 'page': 0, 'page_label': '1', 'source_file': 'DJS Compute DA & ML Task 2.pdf', 'file_type': 'pdf'}, page_content='plots. \\nii) Hierarchical Clustering: Use a dendrogram to visually determine the optimal number of \\nclusters. \\niii) DBSCAN: Use a k-distance plot to find an appropriate value for the eps parameter. \\niv) HDBSCAN: Tune min_cluster_size by setting it to the smallest meaningful group size, \\nor by finding the value that maximizes the DBCV score. \\n● For every clustering experiment you run, calculate a suite of quantitative metrics \\nincluding the Silhouette Score, Davies-Bouldin Index (DBI), and Calinski-Harabasz (CH) \\nScore. \\n● For your single best-performing model, move beyond metrics to interpretation. Profile \\neach cluster by calculating its central tendencies (mean/median for numeric features, \\nmode for categorical features) and visualize these differences to create insightful, \\ndescriptive personas (Ex:- Cluster 1: Cautious High-Earners).'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'Microsoft Word', 'creationdate': '2025-10-05T13:26:17+00:00', 'moddate': '2025-10-05T13:26:17+00:00', 'source': '../data/pdf/DJS Compute DA & ML Task 2.pdf', 'total_pages': 3, 'page': 1, 'page_label': '2', 'source_file': 'DJS Compute DA & ML Task 2.pdf', 'file_type': 'pdf'}, page_content=\"⇒ PART TWO: \\nBefore building the final application, you will validate your understanding of clustering \\nalgorithms by testing them on simple, synthetic datasets where the ground truth is known. \\n● Create synthetic datasets using sklearn functions like make_blobs/make_moons and be \\nsure to experiment with their parameters as well. Heavily reference the sklearn \\ndocumentation for getting familiar with all the parameters. \\n● For more clarity, use visual plots to see how different parameter combinations affect the \\ncluster formations. \\n● STREAMLIT DEPLOYMENT: \\nThe app that you make should reference the synthetic dataset which you created and must \\nhave the following deliverables: \\n○ Feature Scaling Options: Include a select box for preprocessing, allowing a choice \\nbetween different scalers like 'StandardScaler', 'MinMaxScaler', 'RobustScaler', or 'None'. \\n○ Algorithm Choice: Create a dropdown menu for selecting the clustering algorithm (Ex:-\"),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'Microsoft Word', 'creationdate': '2025-10-05T13:26:17+00:00', 'moddate': '2025-10-05T13:26:17+00:00', 'source': '../data/pdf/DJS Compute DA & ML Task 2.pdf', 'total_pages': 3, 'page': 1, 'page_label': '2', 'source_file': 'DJS Compute DA & ML Task 2.pdf', 'file_type': 'pdf'}, page_content=\"between different scalers like 'StandardScaler', 'MinMaxScaler', 'RobustScaler', or 'None'. \\n○ Algorithm Choice: Create a dropdown menu for selecting the clustering algorithm (Ex:-\\n'K-Means', 'DBSCAN', 'Agglomerative', 'HDBSCAN'). \\n○ Dynamic Hyperparameters: Display interactive widgets (sliders are best) for the chosen \\nalgorithm's specific parameters, such as k for K-Means or eps for DBSCAN,etc. \\n○ Interactive Visualization: Render the output as a Plotly scatter plot where each point's \\ncolor corresponds to its assigned cluster label. \\n○ Performance Metrics: Show key evaluation metrics like the Silhouette Score to provide \\na quantitative measure of the clustering quality. \\n- As you wanna view the Plotly scatterplot in 2D/3D make sure to manipulate the - \\n‘n_features’ parameter accordingly while making you synthetic dataset. \\n‘make_moons’ can only create a 2D dataset, so you don’t need to worry about that if \\nyou’re using this function.\"),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'Microsoft Word', 'creationdate': '2025-10-05T13:26:17+00:00', 'moddate': '2025-10-05T13:26:17+00:00', 'source': '../data/pdf/DJS Compute DA & ML Task 2.pdf', 'total_pages': 3, 'page': 1, 'page_label': '2', 'source_file': 'DJS Compute DA & ML Task 2.pdf', 'file_type': 'pdf'}, page_content='‘n_features’ parameter accordingly while making you synthetic dataset. \\n‘make_moons’ can only create a 2D dataset, so you don’t need to worry about that if \\nyou’re using this function. \\n- NOTE : Keep the datapoint range for the synthetic dataset preferably between 500-\\n2000, as we don’t want unnecessary latency in the deployed app. \\n- The Streamlit deployment is only to be done for PART TWO. \\n \\nThe use of comments to explain your code is mandatory. The code can be AI generated. The \\ncomments should not be.'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'Microsoft Word', 'creationdate': '2025-10-05T13:26:17+00:00', 'moddate': '2025-10-05T13:26:17+00:00', 'source': '../data/pdf/DJS Compute DA & ML Task 2.pdf', 'total_pages': 3, 'page': 2, 'page_label': '3', 'source_file': 'DJS Compute DA & ML Task 2.pdf', 'file_type': 'pdf'}, page_content='References: \\nDocumentation: \\nRefer directly from scikit-learn’s website. \\n \\nVideos: \\nPCA : https://www.youtube.com/watch?v=FD4DeN81ODY \\nIntro to Unsupervised : https://www.youtube.com/watch?v=5yeJ03crTrI \\nK-Means : https://www.youtube.com/watch?v=4b5d3muPQmA \\nHierarchical : https://www.youtube.com/watch?v=7xHsRkOdVwo \\nDBSCAN : https://www.youtube.com/watch?v=RDZUdRSDOok \\nHDBSCAN : https://www.youtube.com/watch?v=dGsxd67IFiU \\nElbow Method : https://www.youtube.com/watch?v=ht7geyMAFfA \\nStreamlit Deployment : https://www.youtube.com/watch?v=D0D4Pa22iG0&t=603s \\n \\n \\n \\nDeadline: 14th October, 2025'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'Microsoft Word', 'creationdate': '2025-10-20T17:40:42+00:00', 'moddate': '2025-10-20T17:40:42+00:00', 'source': '../data/pdf/DJS Compute DA & ML Gen AI Task 1.pdf', 'total_pages': 2, 'page': 0, 'page_label': '1', 'source_file': 'DJS Compute DA & ML Gen AI Task 1.pdf', 'file_type': 'pdf'}, page_content='DJS Compute - DA & ML : Gen AI Task 1 \\n \\nLEARNING: \\nThings about LLMS: \\n- Tokenization  \\n- Parameters (Model size)  \\n- Context windows  \\n- System prompt v/s User prompt  \\n- Few shot prompting  \\n \\nWhere to find and run LLMs: \\n- Open source v/s Closed source LLMs - \\nhttps://www.youtube.com/watch?v=710PDpuLwOc \\nhttps://www.youtube.com/watch?v=QQ1bnrTchYM \\n- Huggingface v/s Ollama  \\n- APIs  \\n \\nThe practical side: \\n- API pricing for LLMs (Important)  \\n- Base vs Instruct-tuned LLMs  \\n- Maximizing LLM performance (Very Important) -'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'Microsoft Word', 'creationdate': '2025-10-20T17:40:42+00:00', 'moddate': '2025-10-20T17:40:42+00:00', 'source': '../data/pdf/DJS Compute DA & ML Gen AI Task 1.pdf', 'total_pages': 2, 'page': 1, 'page_label': '2', 'source_file': 'DJS Compute DA & ML Gen AI Task 1.pdf', 'file_type': 'pdf'}, page_content='LLM Leaderboards to check out: \\n- Vellum  \\n- Huggingface OpenLLM  \\n \\n \\nTASK : \\n● Experiment creating a chatbot using different inference methods: \\n- Ollama(locally) \\n- Huggingface transformers (Library) \\n- Gemini/Groq API \\n● REFERENCES : \\n- Ollama \\n- Huggingface Transformers \\n- Groq \\n- Gemini \\n \\nNOTE : The procedure to call and use LLMs even through different \\nmethods is pretty similar and once you do it a couple of times, it’s pretty \\nintuitive. What’s important is knowing the difference between using \\nmodels through UI as a consumer v/s What LLM APIs are, and how \\ntheir inference costs work. After completing this task, also try to \\nintegrate ‘Gradio’ using AI, which is just like Streamlit but for Chatbots. \\n \\nDeadline: 31st October, 2025')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5bde24ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating embeddings for 11 texts...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:01<00:00,  1.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated embeddings with shape: (11, 384)\n",
      "Adding 11 documents to vector store...\n",
      "Successfully added 11 documents to vector store\n",
      "Total documents in collection: 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Convert the text to embeddings\n",
    "texts=[doc.page_content for doc in chunks]\n",
    "\n",
    "# Generate the Embeddings\n",
    "embeddings=embedding_manager.generate_embeddings(texts)\n",
    "\n",
    "# Store in the vector database\n",
    "vectorstore.add_documents(chunks,embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "73c40d73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11, 11, 11)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(texts), len(embeddings), len(chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "498acd10",
   "metadata": {},
   "source": [
    "### Retriever Pipeline From VectorStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0f7b0ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAGRetriever:\n",
    "    \"\"\"Handles query-based retrieval from the vector store\"\"\"\n",
    "    \n",
    "    def __init__(self, vector_store: VectorStore, embedding_manager: EmbeddingManager):\n",
    "        \"\"\"\n",
    "        Initialize the retriever\n",
    "        \n",
    "        Args:\n",
    "            vector_store: Vector store containing document embeddings\n",
    "            embedding_manager: Manager for generating query embeddings\n",
    "        \"\"\"\n",
    "        self.vector_store = vector_store\n",
    "        self.embedding_manager = embedding_manager\n",
    "\n",
    "    def retrieve(self, query: str, top_k: int = 5, score_threshold: float = 0.0) -> List[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Retrieve relevant documents for a query\n",
    "        \n",
    "        Args:\n",
    "            query: The search query\n",
    "            top_k: Number of top results to return\n",
    "            score_threshold: Minimum similarity score threshold\n",
    "            \n",
    "        Returns:\n",
    "            List of dictionaries containing retrieved documents and metadata\n",
    "        \"\"\"\n",
    "        print(f\"Retrieving documents for query: '{query}'\")\n",
    "        print(f\"Top K: {top_k}, Score threshold: {score_threshold}\")\n",
    "        \n",
    "        # Generate query embedding\n",
    "        query_embedding = self.embedding_manager.generate_embeddings([query])[0]\n",
    "        \n",
    "        # Search in vector store\n",
    "        try:\n",
    "            results = self.vector_store.collection.query(\n",
    "                query_embeddings=[query_embedding.tolist()],\n",
    "                n_results=top_k\n",
    "            )\n",
    "            \n",
    "            # Process results\n",
    "            retrieved_docs = []\n",
    "            \n",
    "            if results['documents'] and results['documents'][0]:\n",
    "                documents = results['documents'][0]\n",
    "                metadatas = results['metadatas'][0]\n",
    "                distances = results['distances'][0]\n",
    "                ids = results['ids'][0]\n",
    "                \n",
    "                for i, (doc_id, document, metadata, distance) in enumerate(zip(ids, documents, metadatas, distances)):\n",
    "                    # Convert distance to similarity score (ChromaDB uses cosine distance)\n",
    "                    similarity_score = 1 - distance\n",
    "                    \n",
    "                    if similarity_score >= score_threshold:\n",
    "                        retrieved_docs.append({\n",
    "                            'id': doc_id,\n",
    "                            'content': document,\n",
    "                            'metadata': metadata,\n",
    "                            'similarity_score': similarity_score,\n",
    "                            'distance': distance,\n",
    "                            'rank': i + 1\n",
    "                        })\n",
    "                \n",
    "                print(f\"Retrieved {len(retrieved_docs)} documents (after filtering)\")\n",
    "            else:\n",
    "                print(\"No documents found\")\n",
    "            \n",
    "            return retrieved_docs\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error during retrieval: {e}\")\n",
    "            return []\n",
    "\n",
    "rag_retriever=RAGRetriever(vectorstore,embedding_manager)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "351730b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.RAGRetriever at 0x15df9d7f0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f7e78529",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving documents for query: 'Where to find and run LLMs?'\n",
      "Top K: 5, Score threshold: 0.0\n",
      "Generating embeddings for 1 texts...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  2.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated embeddings with shape: (1, 384)\n",
      "Retrieved 1 documents (after filtering)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'id': 'doc_b7a4f1d7_9',\n",
       "  'content': 'DJS Compute - DA & ML : Gen AI Task 1 \\n \\nLEARNING: \\nThings about LLMS: \\n- Tokenization  \\n- Parameters (Model size)  \\n- Context windows  \\n- System prompt v/s User prompt  \\n- Few shot prompting  \\n \\nWhere to find and run LLMs: \\n- Open source v/s Closed source LLMs - \\nhttps://www.youtube.com/watch?v=710PDpuLwOc \\nhttps://www.youtube.com/watch?v=QQ1bnrTchYM \\n- Huggingface v/s Ollama  \\n- APIs  \\n \\nThe practical side: \\n- API pricing for LLMs (Important)  \\n- Base vs Instruct-tuned LLMs  \\n- Maximizing LLM performance (Very Important) -',\n",
       "  'metadata': {'page': 0,\n",
       "   'producer': 'PyPDF',\n",
       "   'page_label': '1',\n",
       "   'doc_index': 9,\n",
       "   'creator': 'Microsoft Word',\n",
       "   'moddate': '2025-10-20T17:40:42+00:00',\n",
       "   'content_length': 530,\n",
       "   'creationdate': '2025-10-20T17:40:42+00:00',\n",
       "   'source_file': 'DJS Compute DA & ML Gen AI Task 1.pdf',\n",
       "   'total_pages': 2,\n",
       "   'source': '../data/pdf/DJS Compute DA & ML Gen AI Task 1.pdf',\n",
       "   'file_type': 'pdf'},\n",
       "  'similarity_score': 0.05171269178390503,\n",
       "  'distance': 0.948287308216095,\n",
       "  'rank': 1}]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_retriever.retrieve(\"Where to find and run LLMs?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce23783e",
   "metadata": {},
   "source": [
    "### RAG Pipeline : VectorDB To LLM Output Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "449a65c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ba4b617e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.messages import HumanMessage, SystemMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "40bba05b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GroqLLM:\n",
    "    def __init__(self, model_name: str = \"gpt-oss-120b\", api_key: str =None):\n",
    "        \"\"\"\n",
    "        Initialize Groq LLM\n",
    "        \n",
    "        Args:\n",
    "            model_name: Groq model name (qwen2-72b-instruct, llama3-70b-8192, etc.)\n",
    "            api_key: Groq API key (or set GROQ_API_KEY environment variable)\n",
    "        \"\"\"\n",
    "        self.model_name = model_name\n",
    "        self.api_key = api_key or os.environ.get(\"GROQ_API_KEY\")\n",
    "        \n",
    "        if not self.api_key:\n",
    "            raise ValueError(\"Groq API key is required. Set GROQ_API_KEY environment variable or pass api_key parameter.\")\n",
    "        \n",
    "        self.llm = ChatGroq(\n",
    "            groq_api_key=self.api_key,\n",
    "            model_name=self.model_name,\n",
    "            temperature=0.2,\n",
    "            max_tokens=1024\n",
    "        )\n",
    "        \n",
    "        print(f\"Initialized Groq LLM with model: {self.model_name}\")\n",
    "\n",
    "    def generate_response(self, query: str, context: str, max_length: int = 500) -> str:\n",
    "        \"\"\"\n",
    "        Generate response using retrieved context\n",
    "        \n",
    "        Args:\n",
    "            query: User question\n",
    "            context: Retrieved document context\n",
    "            max_length: Maximum response length\n",
    "            \n",
    "        Returns:\n",
    "            Generated response string\n",
    "        \"\"\"\n",
    "        \n",
    "        # Create prompt template\n",
    "        prompt_template = PromptTemplate(\n",
    "            input_variables=[\"context\", \"question\"],\n",
    "            template=\"\"\"You are a helpful AI assistant. Use the following context to answer the question accurately and concisely.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer: Provide a clear and informative answer based on the context above. If the context doesn't contain enough information to answer the question, say so.\"\"\"\n",
    "        )\n",
    "        \n",
    "        # Format the prompt\n",
    "        formatted_prompt = prompt_template.format(context=context, question=query)\n",
    "        \n",
    "        try:\n",
    "            # Generate response\n",
    "            messages = [HumanMessage(content=formatted_prompt)]\n",
    "            response = self.llm.invoke(messages)\n",
    "            return response.content\n",
    "            \n",
    "        except Exception as e:\n",
    "            return f\"Error generating response: {str(e)}\"\n",
    "        \n",
    "    def generate_response_simple(self, query: str, context: str) -> str:\n",
    "        \"\"\"\n",
    "        Simple response generation without complex prompting\n",
    "        \n",
    "        Args:\n",
    "            query: User question\n",
    "            context: Retrieved context\n",
    "            \n",
    "        Returns:\n",
    "            Generated response\n",
    "        \"\"\"\n",
    "        simple_prompt = f\"\"\"Based on this context: {context}\n",
    "\n",
    "Question: {query}\n",
    "\n",
    "Answer:\"\"\"\n",
    "        \n",
    "        try:\n",
    "            messages = [HumanMessage(content=simple_prompt)]\n",
    "            response = self.llm.invoke(messages)\n",
    "            return response.content\n",
    "        except Exception as e:\n",
    "            return f\"Error: {str(e)}\"\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b1fc0f74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized Groq LLM with model: gpt-oss-120b\n",
      "Groq LLM initialized successfully!\n"
     ]
    }
   ],
   "source": [
    "# Initialize Groq LLM (you'll need to set GROQ_API_KEY environment variable)\n",
    "try:\n",
    "    groq_llm = GroqLLM(api_key=os.getenv(\"GROQ_API_KEY\"))\n",
    "    print(\"Groq LLM initialized successfully!\")\n",
    "except ValueError as e:\n",
    "    print(f\"Warning: {e}\")\n",
    "    print(\"Please set your GROQ_API_KEY environment variable to use the LLM.\")\n",
    "    groq_llm = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c4110c55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving documents for query: 'What are things related to LLMs'\n",
      "Top K: 5, Score threshold: 0.0\n",
      "Generating embeddings for 1 texts...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:01<00:00,  1.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated embeddings with shape: (1, 384)\n",
      "Retrieved 1 documents (after filtering)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'id': 'doc_b7a4f1d7_9',\n",
       "  'content': 'DJS Compute - DA & ML : Gen AI Task 1 \\n \\nLEARNING: \\nThings about LLMS: \\n- Tokenization  \\n- Parameters (Model size)  \\n- Context windows  \\n- System prompt v/s User prompt  \\n- Few shot prompting  \\n \\nWhere to find and run LLMs: \\n- Open source v/s Closed source LLMs - \\nhttps://www.youtube.com/watch?v=710PDpuLwOc \\nhttps://www.youtube.com/watch?v=QQ1bnrTchYM \\n- Huggingface v/s Ollama  \\n- APIs  \\n \\nThe practical side: \\n- API pricing for LLMs (Important)  \\n- Base vs Instruct-tuned LLMs  \\n- Maximizing LLM performance (Very Important) -',\n",
       "  'metadata': {'doc_index': 9,\n",
       "   'moddate': '2025-10-20T17:40:42+00:00',\n",
       "   'creationdate': '2025-10-20T17:40:42+00:00',\n",
       "   'source_file': 'DJS Compute DA & ML Gen AI Task 1.pdf',\n",
       "   'content_length': 530,\n",
       "   'creator': 'Microsoft Word',\n",
       "   'total_pages': 2,\n",
       "   'source': '../data/pdf/DJS Compute DA & ML Gen AI Task 1.pdf',\n",
       "   'file_type': 'pdf',\n",
       "   'page': 0,\n",
       "   'page_label': '1',\n",
       "   'producer': 'PyPDF'},\n",
       "  'similarity_score': 0.030491292476654053,\n",
       "  'distance': 0.969508707523346,\n",
       "  'rank': 1}]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### get the context from the retriever and pass it to the LLM\n",
    "\n",
    "rag_retriever.retrieve(\"What are things related to LLMs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ea465ac",
   "metadata": {},
   "source": [
    "### Integration of VectorDB Context pipeline With LLM output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a950a0c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Simple RAG pipeline with Groq LLM\n",
    "from langchain_groq import ChatGroq\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "### Initialize the Groq LLM (set your GROQ_API_KEY in environment)\n",
    "groq_api_key = os.getenv(\"GROQ_API_KEY\")\n",
    "\n",
    "llm=ChatGroq(groq_api_key=groq_api_key,model_name=\"openai/gpt-oss-120b\",temperature=0.1,max_tokens=1024)\n",
    "\n",
    "## 2. Simple RAG function: retrieve context + generate response\n",
    "def rag_simple(query,retriever,llm,top_k=3):\n",
    "    ## retriever the context\n",
    "    results=retriever.retrieve(query,top_k=top_k)\n",
    "    context=\"\\n\\n\".join([doc['content'] for doc in results]) if results else \"\"\n",
    "    if not context:\n",
    "        return \"No relevant context found to answer the question.\"\n",
    "    \n",
    "    ## generate the answwer using GROQ LLM\n",
    "    prompt=f\"\"\"Use the following context to answer the question concisely.\n",
    "        Context:\n",
    "        {context}\n",
    "\n",
    "        Question: {query}\n",
    "\n",
    "        Answer:\"\"\"\n",
    "    \n",
    "    response=llm.invoke([prompt.format(context=context,query=query)])\n",
    "    return response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "df1bf366",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving documents for query: 'What are things related to llms?'\n",
      "Top K: 3, Score threshold: 0.0\n",
      "Generating embeddings for 1 texts...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  9.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated embeddings with shape: (1, 384)\n",
      "Retrieved 1 documents (after filtering)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Things related to LLMs include:\n",
      "\n",
      "- **Tokenization**  \n",
      "- **Parameters (model size)**  \n",
      "- **Context windows**  \n",
      "- **System prompt vs. user prompt**  \n",
      "- **Few‑shot prompting**  \n",
      "\n",
      "- **Open‑source vs. closed‑source models** (e.g., Hugging Face vs. Ollama)  \n",
      "- **APIs for accessing LLMs** and their **pricing**  \n",
      "\n",
      "- **Base models vs. instruction‑tuned (instruct) models**  \n",
      "- **Techniques for maximizing LLM performance**.\n"
     ]
    }
   ],
   "source": [
    "answer=rag_simple(\"What are things related to llms?\",rag_retriever,llm)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6857b1c2",
   "metadata": {},
   "source": [
    "### Enhanced RAG Pipeline Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2832fd17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving documents for query: 'Things related to LLMs'\n",
      "Top K: 3, Score threshold: 0.1\n",
      "Generating embeddings for 1 texts...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  3.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated embeddings with shape: (1, 384)\n",
      "Retrieved 1 documents (after filtering)\n",
      "Answer: **Key concepts and practical aspects of Large Language Models (LLMs)**  \n",
      "\n",
      "- **Tokenization** – converting text into sub‑word tokens that the model processes.  \n",
      "- **Parameters (model size)** – number of trainable weights; larger models generally have higher capability but cost more to run.  \n",
      "- **Context window** – the maximum number of tokens the model can attend to in a single request.  \n",
      "- **System prompt vs. user prompt** – system prompt sets the overall behavior/role; user prompt supplies the specific task or query.  \n",
      "- **Few‑shot prompting** – providing a few example input‑output pairs within the prompt to guide the model without fine‑tuning.  \n",
      "\n",
      "**Where to find/run LLMs**  \n",
      "- **Open‑source vs. closed‑source** models (e.g., LLaMA, Mistral vs. GPT‑4, Claude).  \n",
      "- **Platforms**: Hugging Face (model hub, inference API), Ollama (local serving), cloud APIs (OpenAI, Anthropic, Cohere, etc.).  \n",
      "\n",
      "**Practical considerations**  \n",
      "- **API pricing** – cost per token (input + output) varies by provider; essential for budgeting.  \n",
      "- **Base vs. instruct‑tuned models** – base models are raw; instruct‑tuned are fine‑tuned to follow instructions better out‑of‑the‑box.  \n",
      "- **Maximizing performance** – choose appropriate model size, keep prompts within the context window, use system prompts effectively, employ few‑shot examples sparingly, cache frequent calls, and batch requests when possible.  \n",
      "Sources: [{'source': 'DJS Compute DA & ML Gen AI Task 1.pdf', 'page': 0, 'score': 0.10971313714981079, 'preview': 'DJS Compute - DA & ML : Gen AI Task 1 \\n \\nLEARNING: \\nThings about LLMS: \\n- Tokenization  \\n- Parameters (Model size)  \\n- Context windows  \\n- System prompt v/s User prompt  \\n- Few shot prompting  \\n \\nWhere to find and run LLMs: \\n- Open source v/s Closed source LLMs - \\nhttps://www.youtube.com/watch?v=710...'}]\n",
      "Confidence: 0.10971313714981079\n",
      "Context Preview: DJS Compute - DA & ML : Gen AI Task 1 \n",
      " \n",
      "LEARNING: \n",
      "Things about LLMS: \n",
      "- Tokenization  \n",
      "- Parameters (Model size)  \n",
      "- Context windows  \n",
      "- System prompt v/s User prompt  \n",
      "- Few shot prompting  \n",
      " \n",
      "Where to find and run LLMs: \n",
      "- Open source v/s Closed source LLMs - \n",
      "https://www.youtube.com/watch?v=710\n"
     ]
    }
   ],
   "source": [
    "# --- Enhanced RAG Pipeline Features ---\n",
    "def rag_advanced(query, retriever, llm, top_k=5, min_score=0.2, return_context=False):\n",
    "    \"\"\"\n",
    "    RAG pipeline with extra features:\n",
    "    - Returns answer, sources, confidence score, and optionally full context.\n",
    "    \"\"\"\n",
    "    results = retriever.retrieve(query, top_k=top_k, score_threshold=min_score)\n",
    "    if not results:\n",
    "        return {'answer': 'No relevant context found.', 'sources': [], 'confidence': 0.0, 'context': ''}\n",
    "    \n",
    "    # Prepare context and sources\n",
    "    context = \"\\n\\n\".join([doc['content'] for doc in results])\n",
    "    sources = [{\n",
    "        'source': doc['metadata'].get('source_file', doc['metadata'].get('source', 'unknown')),\n",
    "        'page': doc['metadata'].get('page', 'unknown'),\n",
    "        'score': doc['similarity_score'],\n",
    "        'preview': doc['content'][:300] + '...'\n",
    "    } for doc in results]\n",
    "    confidence = max([doc['similarity_score'] for doc in results])\n",
    "    \n",
    "    # Generate answer\n",
    "    prompt = f\"\"\"Use the following context to answer the question concisely.\\nContext:\\n{context}\\n\\nQuestion: {query}\\n\\nAnswer:\"\"\"\n",
    "    response = llm.invoke([prompt.format(context=context, query=query)])\n",
    "    \n",
    "    output = {\n",
    "        'answer': response.content,\n",
    "        'sources': sources,\n",
    "        'confidence': confidence\n",
    "    }\n",
    "    if return_context:\n",
    "        output['context'] = context\n",
    "    return output\n",
    "\n",
    "# Example usage:\n",
    "result = rag_advanced(\"Things related to LLMs\", rag_retriever, llm, top_k=3, min_score=0.1, return_context=True)\n",
    "print(\"Answer:\", result['answer'])\n",
    "print(\"Sources:\", result['sources'])\n",
    "print(\"Confidence:\", result['confidence'])\n",
    "print(\"Context Preview:\", result['context'][:300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "aa6150d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving documents for query: 'things related to LLMs'\n",
      "Top K: 3, Score threshold: 0.1\n",
      "Generating embeddings for 1 texts...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated embeddings with shape: (1, 384)\n",
      "Retrieved 1 documents (after filtering)\n",
      "Streaming answer:\n",
      "Use the following context to answer the question concisely.\n",
      "Context:\n",
      "DJS Compute"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - DA & ML : Gen AI Task 1 \n",
      " \n",
      "LEARNING: \n",
      "Things about LLMS: \n",
      "- Tokenization  \n",
      "- Parameters (Model size)  \n",
      "- Context windows  \n",
      "- System prompt v/s User prompt  \n",
      "- Few shot prompting  \n",
      " \n",
      "Where to find and run LLMs: \n",
      "- Open source v/s Closed source LLMs - \n",
      "https://www.youtube.com/watch?v=710PDpuLwOc \n",
      "https://www.youtube.com/watch?v=QQ1bnrTchYM \n",
      "- Huggingface v/s Ollama  \n",
      "- APIs  \n",
      " \n",
      "The practical side: \n",
      "- API pricing for LLMs (Important)  \n",
      "- Base vs Instruct-tuned LLMs  \n",
      "- Maximizing LLM performance (Very Important) -\n",
      "\n",
      "Question: things related to LLMs\n",
      "\n",
      "Answer:\n",
      "\n",
      "Final Answer: **Key concepts and practical aspects of Large Language Models (LLMs)**  \n",
      "\n",
      "- **Tokenization** – converting text into sub‑word units that the model processes.  \n",
      "- **Parameters (model size)** – the number of learnable weights; larger models generally have higher capability but cost more to run.  \n",
      "- **Context window** – the maximum number of tokens the model can attend to in a single request.  \n",
      "- **Prompts**  \n",
      "  - *System prompt*: sets the overall behavior/role of the assistant.  \n",
      "  - *User prompt*: the actual query or instruction.  \n",
      "- **Few‑shot prompting** – providing a few example input‑output pairs within the prompt to guide the model’s response.  \n",
      "\n",
      "**Where to find/run LLMs**  \n",
      "- **Open‑source vs. closed‑source** models (e.g., LLaMA, Mistral vs. GPT‑4, Claude).  \n",
      "- **Platforms**: Hugging Face (model hub, inference API) vs. Ollama (local deployment).  \n",
      "- **APIs** – cloud endpoints (OpenAI, Anthropic, Cohere, etc.) for easy integration.  \n",
      "\n",
      "**Practical considerations**  \n",
      "- **API pricing** – cost per token (input + output) varies by provider; essential for budgeting.  \n",
      "- **Base vs. instruct‑tuned models** – base models are raw; instruct‑tuned are fine‑tuned for following instructions and safer responses.  \n",
      "- **Maximizing performance**  \n",
      "  - Choose an appropriate model size for the task.  \n",
      "  - Use system prompts and few‑shot examples to steer behavior.  \n",
      "  - Keep prompts within the context window to avoid truncation.  \n",
      "  - Cache frequent queries, batch requests, and leverage quantization or LoRA adapters for cheaper inference.  \n",
      "\n",
      "These points capture the main “things related to LLMs” highlighted in the provided context.\n",
      "\n",
      "Citations:\n",
      "[1] DJS Compute DA & ML Gen AI Task 1.pdf (page 0)\n",
      "Summary: Large language models work by tokenizing text, using millions to billions of parameters within a limited context window, and are guided by system prompts, user prompts, and few‑shot examples; they come in open‑source (e.g., LLaMA, Mistral) and closed‑source (e.g., GPT‑4, Claude) forms and can be accessed via platforms like Hugging Face, Ollama, or cloud APIs. Practical use requires balancing model size, prompt design, context limits, and cost considerations (pricing, inference efficiency, and tuning) to achieve optimal performance and budget control.\n",
      "History: {'question': 'things related to LLMs', 'answer': '**Key concepts and practical aspects of Large Language Models (LLMs)**  \\n\\n- **Tokenization** – converting text into sub‑word units that the model processes.  \\n- **Parameters (model size)** – the number of learnable weights; larger models generally have higher capability but cost more to run.  \\n- **Context window** – the maximum number of tokens the model can attend to in a single request.  \\n- **Prompts**  \\n  - *System prompt*: sets the overall behavior/role of the assistant.  \\n  - *User prompt*: the actual query or instruction.  \\n- **Few‑shot prompting** – providing a few example input‑output pairs within the prompt to guide the model’s response.  \\n\\n**Where to find/run LLMs**  \\n- **Open‑source vs. closed‑source** models (e.g., LLaMA, Mistral vs. GPT‑4, Claude).  \\n- **Platforms**: Hugging\\u202fFace (model hub, inference API) vs. Ollama (local deployment).  \\n- **APIs** – cloud endpoints (OpenAI, Anthropic, Cohere, etc.) for easy integration.  \\n\\n**Practical considerations**  \\n- **API pricing** – cost per token (input\\u202f+\\u202foutput) varies by provider; essential for budgeting.  \\n- **Base vs. instruct‑tuned models** – base models are raw; instruct‑tuned are fine‑tuned for following instructions and safer responses.  \\n- **Maximizing performance**  \\n  - Choose an appropriate model size for the task.  \\n  - Use system prompts and few‑shot examples to steer behavior.  \\n  - Keep prompts within the context window to avoid truncation.  \\n  - Cache frequent queries, batch requests, and leverage quantization or LoRA adapters for cheaper inference.  \\n\\nThese points capture the main “things related to LLMs” highlighted in the provided context.', 'sources': [{'source': 'DJS Compute DA & ML Gen AI Task 1.pdf', 'page': 0, 'score': 0.10971313714981079, 'preview': 'DJS Compute - DA & ML : Gen AI Task 1 \\n \\nLEARNING: \\nThings about LLMS: \\n- Tokenization  \\n- Parameters (Model size)  \\n- C...'}], 'summary': 'Large language models work by tokenizing text, using millions to billions of parameters within a limited context window, and are guided by system prompts, user prompts, and few‑shot examples; they come in open‑source (e.g., LLaMA, Mistral) and closed‑source (e.g., GPT‑4, Claude) forms and can be accessed via platforms like Hugging\\u202fFace, Ollama, or cloud APIs. Practical use requires balancing model size, prompt design, context limits, and cost considerations (pricing, inference efficiency, and tuning) to achieve optimal performance and budget control.'}\n"
     ]
    }
   ],
   "source": [
    "# --- Advanced RAG Pipeline: Streaming, Citations, History, Summarization ---\n",
    "from typing import List, Dict, Any\n",
    "import time\n",
    "\n",
    "class AdvancedRAGPipeline:\n",
    "    def __init__(self, retriever, llm):\n",
    "        self.retriever = retriever\n",
    "        self.llm = llm\n",
    "        self.history = []  # Store query history\n",
    "\n",
    "    def query(self, question: str, top_k: int = 5, min_score: float = 0.2, stream: bool = False, summarize: bool = False) -> Dict[str, Any]:\n",
    "        # Retrieve relevant documents\n",
    "        results = self.retriever.retrieve(question, top_k=top_k, score_threshold=min_score)\n",
    "        if not results:\n",
    "            answer = \"No relevant context found.\"\n",
    "            sources = []\n",
    "            context = \"\"\n",
    "        else:\n",
    "            context = \"\\n\\n\".join([doc['content'] for doc in results])\n",
    "            sources = [{\n",
    "                'source': doc['metadata'].get('source_file', doc['metadata'].get('source', 'unknown')),\n",
    "                'page': doc['metadata'].get('page', 'unknown'),\n",
    "                'score': doc['similarity_score'],\n",
    "                'preview': doc['content'][:120] + '...'\n",
    "            } for doc in results]\n",
    "            # Streaming answer simulation\n",
    "            prompt = f\"\"\"Use the following context to answer the question concisely.\\nContext:\\n{context}\\n\\nQuestion: {question}\\n\\nAnswer:\"\"\"\n",
    "            if stream:\n",
    "                print(\"Streaming answer:\")\n",
    "                for i in range(0, len(prompt), 80):\n",
    "                    print(prompt[i:i+80], end='', flush=True)\n",
    "                    time.sleep(0.05)\n",
    "                print()\n",
    "            response = self.llm.invoke([prompt.format(context=context, question=question)])\n",
    "            answer = response.content\n",
    "\n",
    "        # Add citations to answer\n",
    "        citations = [f\"[{i+1}] {src['source']} (page {src['page']})\" for i, src in enumerate(sources)]\n",
    "        answer_with_citations = answer + \"\\n\\nCitations:\\n\" + \"\\n\".join(citations) if citations else answer\n",
    "\n",
    "        # Optionally summarize answer\n",
    "        summary = None\n",
    "        if summarize and answer:\n",
    "            summary_prompt = f\"Summarize the following answer in 2 sentences:\\n{answer}\"\n",
    "            summary_resp = self.llm.invoke([summary_prompt])\n",
    "            summary = summary_resp.content\n",
    "\n",
    "        # Store query history\n",
    "        self.history.append({\n",
    "            'question': question,\n",
    "            'answer': answer,\n",
    "            'sources': sources,\n",
    "            'summary': summary\n",
    "        })\n",
    "\n",
    "        return {\n",
    "            'question': question,\n",
    "            'answer': answer_with_citations,\n",
    "            'sources': sources,\n",
    "            'summary': summary,\n",
    "            'history': self.history\n",
    "        }\n",
    "\n",
    "# Example usage:\n",
    "adv_rag = AdvancedRAGPipeline(rag_retriever, llm)\n",
    "result = adv_rag.query(\"things related to LLMs\", top_k=3, min_score=0.1, stream=True, summarize=True)\n",
    "print(\"\\nFinal Answer:\", result['answer'])\n",
    "print(\"Summary:\", result['summary'])\n",
    "print(\"History:\", result['history'][-1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Complete RAG",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
